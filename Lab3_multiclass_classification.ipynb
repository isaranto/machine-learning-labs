{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab3 - Machine Learning\n",
    "## Multiclass Classification \n",
    "(lec3.pdf, slides 67-72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function (logLikelihood plus reguralization term) we want to maximize for the problem of classifying N number of data in K categories/classes is:\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\sum_{k=1}^K t_{nk} \\log y_{nk}   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w_k}||^2, \n",
    "$$\n",
    "\n",
    "where $y_{nk}$ is the softmax function defined as:\n",
    "\n",
    "$$y_{nk} = \\frac{e^{\\mathbf{w}_k^T \\mathbf{x}_n}}{\\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{x}_n}}$$\n",
    "$W$ is a $K \\times (D+1)$ matrix where each line represents the vector $\\mathbf{w}_k$.\n",
    "\n",
    "\n",
    "The cost function can be simplified in the following form:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "E(W) = \\sum_{n=1}^N \\left[ \\left( \\sum_{k=1}^K t_{nk} \\mathbf{w}_k^T \\mathbf{x}_n \\right) - \\log \\left( \\sum_{j=1}^K e^{\\mathbf{w}_j^T \\mathbf{x}_n} \\right) \\right]   -  \\frac{\\lambda}{2} \\sum_{k=1}^K ||\\mathbf{w}_k||^2, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "In the above formula we have used the fact that $\\sum_{k=1}^K t_{nk} = 1$. \n",
    "\n",
    "The partial derrivatives of this function are given by the following $K \\times (D+1)$ matrix:\n",
    "\n",
    "$$\n",
    "(T - S)^Î¤ X - \\lambda W,\n",
    "$$\n",
    "\n",
    "where $T$ is an $N \\times K$ matrix with the truth values of the training data, such that $[T]_{nk} = t_{nk}$, $S$ is the corresponding $N \\times K$ matrix that holds the softmax probabilities such that $[S]_{nk} = y_{nk}$ and $X$ is the $N \\times (D + 1)$ matrix of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    max_of_rows = np.max(y, 1)\n",
    "    m = np.array([max_of_rows, ] * x.shape[1]).T\n",
    "    y = y - m\n",
    "    y = np.exp(y)\n",
    "    return y / (np.array([np.sum(y, 1), ] * y.shape[1])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the MNIST dataset. Reads the training files and creates matrices.\n",
    "    :return: train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    train_truth: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    test_truth: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    train_files = ['data/mnist/train%d.txt' % (i,) for i in range(10)]\n",
    "    test_files = ['data/mnist/test%d.txt' % (i,) for i in range(10)]\n",
    "    tmp = []\n",
    "    for i in train_files:\n",
    "        with open(i, 'r') as fp:\n",
    "            tmp += fp.readlines()\n",
    "    # load train data in N*D array (60000x784 for MNIST) \n",
    "    #                              divided by 255 to achieve normalization\n",
    "    train_data = np.array([[j for j in i.split(\" \")] for i in tmp], dtype='int') / 255\n",
    "    print \"Train data array size: \", train_data.shape\n",
    "    tmp = []\n",
    "    for i in test_files:\n",
    "        with open(i, 'r') as fp:\n",
    "            tmp += fp.readlines()\n",
    "    # load test data in N*D array (10000x784 for MNIST) \n",
    "    #                             divided by 255 to achieve normalization\n",
    "    test_data = np.array([[j for j in i.split(\" \")] for i in tmp], dtype='int') / 255\n",
    "    print \"Test data array size: \", test_data.shape\n",
    "    tmp = []\n",
    "    for i, _file in enumerate(train_files):\n",
    "        with open(_file, 'r') as fp:\n",
    "            for line in fp:\n",
    "                tmp.append([1 if j == i else 0 for j in range(0, 10)])\n",
    "    train_truth = np.array(tmp, dtype='int')\n",
    "    del tmp[:]\n",
    "    for i, _file in enumerate(test_files):\n",
    "        with open(_file, 'r') as fp:\n",
    "            for _ in fp:\n",
    "                tmp.append([1 if j == i else 0 for j in range(0, 10)])\n",
    "    test_truth = np.array(tmp, dtype='int')\n",
    "    print \"Train truth array size: \", train_truth.shape\n",
    "    print \"Test truth array size: \", test_truth.shape\n",
    "    return train_data, test_data, train_truth, test_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "# plot 5 random images from the training set\n",
    "samples = np.random.randint(X_train.shape[0], size=5)\n",
    "for i in samples:\n",
    "    im = Image.fromarray(X_train[i].reshape(28,28)*255)\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "X_train = np.hstack((np.ones((X_train.shape[0],1)), X_train))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)), X_test))\n",
    "print \"Train truth array size (with ones): \", X_train.shape\n",
    "print \"Test truth array size (with ones): \", X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_train(t, X, lamda, Winit, options):\n",
    "    \"\"\"inputs :\n",
    "      t: N x 1 binary output data vector indicating the two classes\n",
    "      X: N x (D+1) input data vector with ones already added in the first column\n",
    "      lamda: the positive regularizarion parameter\n",
    "      winit: D+1 dimensional vector of the initial values of the parameters\n",
    "      options: options(1) is the maximum number of iterations\n",
    "               options(2) is the tolerance\n",
    "               options(3) is the learning rate eta\n",
    "    outputs :\n",
    "      w: the trained D+1 dimensional vector of the parameters\"\"\"\n",
    "\n",
    "    W = Winit\n",
    "\n",
    "    # Maximum number of iteration of gradient ascend\n",
    "    _iter = options[0]\n",
    "\n",
    "    # Tolerance\n",
    "    tol = options[1]\n",
    "\n",
    "    # Learning rate\n",
    "    eta = options[2]\n",
    "\n",
    "    Ewold = -np.inf\n",
    "    costs = []\n",
    "    for i in range(_iter):\n",
    "        # Call the cost function to compute both the value of the cost\n",
    "        # and its gradients. You should store the value of the cost to \n",
    "        # the variable Ew and the gradients to a K x (D+1) matrix gradEw\n",
    "        \n",
    "        # ******************************************************************\n",
    "        # **********************Your code here *****************************\n",
    "        # ******************************************************************\n",
    "        \n",
    "        # add Ew value to the \"costs\" list\n",
    "        \n",
    "        # ******************************************************************\n",
    "        # **********************Your code here *****************************\n",
    "        # ******************************************************************\n",
    "        \n",
    "        # Show the current cost function on screen\n",
    "        print('iteration %d' % i)\n",
    "        print('cost function :', Ew)\n",
    "\n",
    "        # Break if you achieve the desired accuracy in the cost function\n",
    "        if np.abs(Ew - Ewold) < tol:\n",
    "            break\n",
    "\n",
    "                \n",
    "        # Update parameters based on gradient ascend\n",
    "        # ******************************************************************\n",
    "        # **********************Your code here *****************************\n",
    "        # ******************************************************************\n",
    "\n",
    "        Ewold = Ew\n",
    "\n",
    "    return W, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the cost function below we use the logsumexp trick\n",
    "\\begin{align} \n",
    "\\log \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n} &= \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n +m -m}\\Bigl) \\\\ \n",
    "&= \\log \\Bigr( \\sum_{i=1}^{n} e^m e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\Bigr( e^m \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= \\log \\ e^m + \\log \\Bigr( \\sum_{i=1}^{n} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m} ) \\Bigl) \\\\ \n",
    "&= m + \\log \\Bigr( \\sum_{i=1} e^{\\mathbf{w}_j^T \\mathbf{x}_n-m}  \\Bigl) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cost_grad_softmax(W, X, t, lamda):\n",
    "    # Compute all terms w_k^T * x_n and store them in a N x K matrix Yx\n",
    "    \n",
    "    # ******************************************************************\n",
    "    # **********************Your code here *****************************\n",
    "    # ******************************************************************\n",
    "    \n",
    "    # Compute all softmax probabilities and store them in a N x K matrix S\n",
    "    \n",
    "    # ******************************************************************\n",
    "    # **********************Your code here *****************************\n",
    "    # ******************************************************************\n",
    "    \n",
    "    # Compute the cost function to check convergence\n",
    "    # Using the logsumexp trick for numerical stability - lec8.pdf slide 43\n",
    "    max_error = np.max(s, axis=1)\n",
    "    Ew = np.sum(t * y) - np.sum(max_error) - \\\n",
    "        np.sum(np.log(np.sum(np.exp(y - np.array([max_error, ] * y.shape[1]).T), 1))) - \\\n",
    "        (0.5 * lamda) * np.sum(np.square(W))  \n",
    "\n",
    "    # calculate gradient\n",
    "    \n",
    "    # ******************************************************************\n",
    "    # **********************Your code here *****************************\n",
    "    # ******************************************************************\n",
    "    \n",
    "    return Ew, gradEw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient checking\n",
    "\n",
    "lec4_b.pdf - Slide 20\n",
    "\n",
    "\n",
    "During gradient ascent/descent we compute the gradients $\\frac{\\partial E}{\\partial w}$, where $w$ denotes the parameters of the model.\n",
    "\n",
    "In order to make sure that these gradients are correct we will compare the exact gradients(that we have coded) with numerical estimates obtained by finite differences, you can use your code for computing $E$ to verify the code for computing $\\frac{\\partial E}{\\partial w}$.\n",
    "    Let's look back at the definition of a derivative (or gradient):\n",
    "    \n",
    "$$ \\frac{\\partial E}{\\partial w} = \\lim_{\\varepsilon \\to 0} \\frac{E(w + \\varepsilon) - E(w - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$  \n",
    "\n",
    "We know the following: \n",
    "- $\\frac{\\partial E}{\\partial w}$ is what you want to make sure you're computing correctly. ,\n",
    "- You can compute $E(w + \\varepsilon)$ and $E(w - \\varepsilon)$ (in the case that $w$ is a real number), since you're confident your implementation for $E$ is correct.\n",
    "\n",
    "Lets use equation (1) and a small value for $\\varepsilon$ to make sure that your code for computing  $\\frac{\\partial E}{\\partial w}$ is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_softmax(Winit, X, t, lamda):\n",
    "    W = np.random.rand(*Winit.shape)\n",
    "    epsilon = 1e-6\n",
    "    _list = np.random.randint(X.shape[0], size=5)\n",
    "    x_sample = np.array(X[_list, :])\n",
    "    t_sample = np.array(t[_list, :])\n",
    "    \n",
    "    # Compute the analytic gradients and store them in gradEw\n",
    "    \n",
    "    # ******************************************************************\n",
    "    # **********************Your code here *****************************\n",
    "    # ******************************************************************\n",
    "    \n",
    "    \n",
    "    print \"gradEw shape: \", gradEw.shape\n",
    "    numericalGrad = np.zeros(gradEw.shape)\n",
    "    # Compute all numerical gradient estimates and store them in\n",
    "    # the matrix numericalGrad\n",
    "    for k in range(numericalGrad.shape[0]):\n",
    "        for d in range(numericalGrad.shape[1]):\n",
    "            \n",
    "        # ******************************************************************\n",
    "        # **********************Your code here *****************************\n",
    "        # ******************************************************************\n",
    "    \n",
    "    \n",
    "    # Absolute norm\n",
    "    print \"The difference estimate for gradient of w is : \", np.max(np.abs(gradEw - numericalGrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ml_softmax_test(W, X_test):\n",
    "    ytest = softmax(X_test.dot(W.T))\n",
    "    # Hard classification decisions\n",
    "    ttest = np.argmax(ytest, 1)\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# N of X\n",
    "N, D = X_train.shape\n",
    "\n",
    "K = 10\n",
    "\n",
    "# initialize w for the gradient ascent\n",
    "Winit = np.zeros((K, D))\n",
    "\n",
    "# regularization parameter\n",
    "lamda = 0\n",
    "\n",
    "# options for gradient descent\n",
    "options = [500, 1e-6, 0.5/N]\n",
    "\n",
    "gradcheck_softmax(Winit, X_train, y_train, lamda)\n",
    "\n",
    "# Train the model\n",
    "# W, costs = ml_softmax_train(y_train, X_train, lamda, Winit, options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(format(options[2], 'f')))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttest = ml_softmax_test(W, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_count = np.not_equal(np.argmax(y_test,1), ttest).sum()\n",
    "print \"Error is \", error_count / y_test.shape[0] * 100, \" %\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at our test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "faults = np.where(np.not_equal(np.argmax(y_test,1),ttest))[0]\n",
    "# plot 10 misclassified examples from the Test set\n",
    "samples = np.random.choice(faults, 10)\n",
    "for i in samples:\n",
    "    im = Image.fromarray(X_test[i,1:].reshape(28,28)*255)\n",
    "    plt.figure()\n",
    "    plt.imshow(im)\n",
    "    plt.title(\"Truth: \"+str(np.argmax(y_test,1)[i])+ \", Predicted: \"+ str(ttest[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
