{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab4 - Machine Learning\n",
    "## Classification with KNN and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. k-nearest-neighbors (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data_subset():\n",
    "    \"\"\"\n",
    "    Load subsets from MNIST for 5 and 9\"\"\"\n",
    "    data = scipy.io.loadmat('data/mnist/mnistSubset5and9.mat')\n",
    "    X_train = np.vstack([data['train5'],data['train9']])/255\n",
    "    X_test = np.vstack([data['test5'],data['test9']])/255\n",
    "    y_train = np.vstack([np.array([[1,0] for _ in range(data['train5'].shape[0])]),\n",
    "                         np.array([[0,1] for _ in range(data['train9'].shape[0])])])\n",
    "    y_test = np.vstack([np.array([[1,0] for _ in range(data['test5'].shape[0])]),\n",
    "                         np.array([[0,1] for _ in range(data['test9'].shape[0])])])\n",
    "    print \"Train data array size: \", X_train.shape    \n",
    "    print \"Train truth array size: \", y_train.shape\n",
    "    print \"Test data array size: \", X_test.shape\n",
    "    print \"Test truth array size: \", y_test.shape\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data_subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sqrdist(x, y):\n",
    "    \"\"\" X is a matrix (N x D) and y is a vector (1 x D)\n",
    "    \"\"\"\n",
    "    y = np.tile(y,(x.shape[0],1))\n",
    "    d = np.sqrt((np.square(x-y).sum(axis=1)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you should implement the knn_test function which takes as input a $N \\times D$ matrix $X$ with the input data, a $N \\times K$ matrix $T$ which holds the one hot vectors that are the ground truth for $X$. Inside the function you should compute for each instance in Xtest, the k-nearest-neighbors and take a decision for the class using the majority rule. \n",
    "\n",
    "Afterwards you can decide for the best K using cross validation and run knn_test for the test_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def knn_test(X, T, Xtest, K):\n",
    "    ##########################################################\n",
    "    ######################Your code Here######################\n",
    "    ##########################################################\n",
    "    return Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(K, numFolds, X, T):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    #random permute the data before we split them\n",
    "    np.random.seed(10)\n",
    "    perm = np.random.permutation(N)\n",
    "    X = X[perm]\n",
    "    T = T[perm]\n",
    "    # The fold variable will have size numFolds + 1\n",
    "    chunk_size = int(N/numFolds)\n",
    "    split_ind = np.arange(chunk_size,chunk_size*numFolds, chunk_size)\n",
    "    if chunk_size*numFolds < N:\n",
    "        split_ind[-1] = N-1\n",
    "    else:\n",
    "        last_chunk = chunk_size\n",
    "    Xfolds = np.vsplit(X, split_ind)\n",
    "    Tfolds = np.vsplit(T, split_ind)\n",
    "    valerr = np.zeros((K, numFolds))\n",
    "    for k in range(1, K+1):\n",
    "        for j in range(numFolds):\n",
    "            X_train = np.vstack([Xfolds[i] for i in range(len(Xfolds)) if i != j])\n",
    "            y_train = np.vstack([Tfolds[i] for i in range(len(Tfolds)) if i != j])\n",
    "            X_test = Xfolds[j]\n",
    "            # prediction using kNN\n",
    "            Ttest = knn_test(X_train, y_train, X_test, k)\n",
    "            # Compute the percent misclassified data points \n",
    "            Tval = np.argmax(Ttest, axis=1)\n",
    "            Tcor = np.argmax(Tfolds[j], axis=1)\n",
    "            valerr[k-1, j] = np.count_nonzero(Tval!=Tcor)/X_test.shape[0]\n",
    "            print(\"kNN for k = {}, fold # {}, error = {} \".format(k, j, valerr[k-1, j]))\n",
    "    # average the validation errors\n",
    "    val = valerr.sum(axis=1)/numFolds\n",
    "    # select best k\n",
    "    kbest = np.argmin(val)+1\n",
    "    # Plot the evolution of the validation error with respect to k\n",
    "    plt.plot(range(1, K+1), val*100)\n",
    "    plt.ylabel('Average Validation Error %')\n",
    "    plt.xlabel('K Nearest Neighbors')\n",
    "    plt.show()\n",
    "    print(\"Best k is {} with average error = {} \".format(kbest, val[kbest]))\n",
    "    return kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kbest = cross_validation(20, 10, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Ttest = knn_test(X_train, y_train, X_test, kbest)\n",
    "\n",
    "Tval = np.argmax(Ttest, axis=1)\n",
    "Tcor = np.argmax(y_test, axis=1)\n",
    "print np.count_nonzero(Tval!=Tcor)/X_test.shape[0]\n",
    "print \"Error is \", np.count_nonzero(Tval!=Tcor)/X_test.shape[0] * 100, \" %\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check slides of Lec5.pdf - slides 20-56\n",
    "\n",
    "Check Bishop - Pattern Recognition and Machine Learning Chapter 4.2.3 - Generative Models with Discrete features - Naive Bayes.\n",
    "\n",
    "In Discriminative models given data $\\mathbf{X}$ we want to calculate the probability that it belongs to a class. So we are trying to model $P(\\mathcal{C_k} \\mid \\mathbf{X})$, which actually is finding the boundaries between different classes. \n",
    "\n",
    "In Generative models we want to model the joint distribution of each class $P(\\mathbf{X}, \\, \\mathcal{C_k})$.\n",
    "\n",
    "Given an instance $\\mathbf{x}$ we want to calculate $ P(\\mathcal{C_k} \\mid \\mathbf{x})$ for each $k \\in K$ and choose the category/class that holds the highest probability.\n",
    "\n",
    "\n",
    "We will use Bayes Rule to calculate what the probability is \n",
    "$$\n",
    "P(\\mathcal{C_k} \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid \\mathcal{C_k}) \\, P(\\mathcal{C_k})}{P(\\mathbf{x})} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The term \"Naive\" is adopted because of the naive assumption of independence between every pair of features. Given a class variable y and a dependent feature vector x<sub>1</sub> through x<sub>n</sub>, Bayesâ€™ theorem states the following relationship:\n",
    "\n",
    "$$P(C_k \\mid x_1, \\dots, x_n) = \\frac{P(x_1, \\dots x_n \\mid C_k)\\, P(\\mathcal{C_k})}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 \n",
    "Using the naive independence assumption that\n",
    "$$ P(x_i \\mid \\mathcal{C_k}, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i \\mid \\mathcal{C_k})$$\n",
    "\n",
    "for all i, this relationship is simplified to:\n",
    "\n",
    "$$P(C_k\\mid x_1, \\dots, x_n) = \\frac{\\prod_{i=1}^{n} P(x_i \\mid y) \\,  P(\\mathcal{C_k})}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "\n",
    "\n",
    "With respect to the above \"naive\" assumption for (1)  we have use a Bernoulli distribution for each of the K classes:\n",
    "$$\n",
    "p(\\mathbf{x} \\mid  \\mathcal{C_k}) = \\prod_{d=1}^D \\mu_{k,d}^{x_d} \\, (1- \\mu_{k,d})^{1 - x_d}\n",
    "$$\n",
    "and \n",
    "$$ P(\\mathbf{x}) = \\sum_{j=1}^K P(\\mathbf{x} \\mid \\mathcal{C_j}) \\, P(\\mathcal{C_j}) $$\n",
    "\n",
    "Although we want to calculate $ P(\\mathcal{C_k} \\mid \\mathbf{x})$, Naive Bayes is a Generative Model because capture $P(\\mathbf{x} \\mid \\mathcal{C_k}) \\, P(\\mathcal{C_k})$ which equals to $P(\\mathbf{X}, \\, \\mathcal{C_k})$.\n",
    "\n",
    "Since the denominator is common for all the classifiers for a given $\\mathbf{x}$ we need to calculate all the nominators and compare them. Optionally you can use the softmax function to convert $\\mathcal{L(\\mu)}$ to real probabilities. The likelihood becomes:\n",
    "$$ \\mathcal{L(\\mu_k)} = \\ln \\left(p(\\mathbf{x} \\mid \\mathcal{C_k})P(\\mathcal{C_k})\\right)= \\sum_{n=1}^N \\sum_{d=1}^D x_{nd}\\,\\ln\\mu_{k,d} + (1 - x_nd)\\ln(1- \\mu_{k,d})$$\n",
    "\n",
    "By differentiating the above log likelihood with respect to $\\mu_{k,d}$ and equating to zero we obtain the optimal parameters\n",
    "$$\\mu_{k,d} = \\frac{\\sum_{n \\in X_k}  x_{n,d}}{N_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads the MNIST dataset. Reads the training files and creates matrices.\n",
    "    :return: train_data:the matrix with the training data\n",
    "    test_data: the matrix with the data that will be used for testing\n",
    "    train_truth: the matrix consisting of one \n",
    "                        hot vectors on each row(ground truth for training)\n",
    "    test_truth: the matrix consisting of one\n",
    "                        hot vectors on each row(ground truth for testing)\n",
    "    \"\"\"\n",
    "    train_files = ['data/mnist/train%d.txt' % (i,) for i in range(10)]\n",
    "    test_files = ['data/mnist/test%d.txt' % (i,) for i in range(10)]\n",
    "    tmp = []\n",
    "    for i in train_files:\n",
    "        with open(i, 'r') as fp:\n",
    "            tmp += fp.readlines()\n",
    "    # load train data in N*D array (60000x784 for MNIST) \n",
    "    #                              divided by 255 to achieve normalization\n",
    "    train_data = np.array([[j for j in i.split(\" \")] for i in tmp], dtype='int') / 255\n",
    "    print \"Train data array size: \", train_data.shape\n",
    "    tmp = []\n",
    "    for i in test_files:\n",
    "        with open(i, 'r') as fp:\n",
    "            tmp += fp.readlines()\n",
    "    # load test data in N*D array (10000x784 for MNIST) \n",
    "    #                             divided by 255 to achieve normalization\n",
    "    test_data = np.array([[j for j in i.split(\" \")] for i in tmp], dtype='int') / 255\n",
    "    print \"Test data array size: \", test_data.shape\n",
    "    tmp = []\n",
    "    for i, _file in enumerate(train_files):\n",
    "        with open(_file, 'r') as fp:\n",
    "            for line in fp:\n",
    "                tmp.append([1 if j == i else 0 for j in range(0, 10)])\n",
    "    train_truth = np.array(tmp, dtype='int')\n",
    "    del tmp[:]\n",
    "    for i, _file in enumerate(test_files):\n",
    "        with open(_file, 'r') as fp:\n",
    "            for _ in fp:\n",
    "                tmp.append([1 if j == i else 0 for j in range(0, 10)])\n",
    "    test_truth = np.array(tmp, dtype='int')\n",
    "    print \"Train truth array size: \", train_truth.shape\n",
    "    print \"Test truth array size: \", test_truth.shape\n",
    "    return train_data, test_data, train_truth, test_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def softmax(y):\n",
    "    max_of_rows = np.max(y, 1)\n",
    "    m = np.array([max_of_rows, ] * y.shape[1]).T\n",
    "    y = y - m\n",
    "    y = np.exp(y)\n",
    "    return y / (np.array([np.sum(y, 1), ] * y.shape[1])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ml_nb_train(X, y):\n",
    "    \"\"\"\n",
    "    Trains Naive Vayes classifier for binary input data\"\"\"\n",
    "    ##########################################################\n",
    "    ######################Your code Here######################\n",
    "    ##########################################################\n",
    "    return m, pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def log_bernoulli(X, mu):\n",
    "    N = X.shape[0]\n",
    "    mu[mu<=0] = 1e-100\n",
    "    mu[mu>=1] = 1 - 1e-100\n",
    "    logPr = np.sum(X*np.log(mu) + (1-X)*np.log(1-mu), axis=1)\n",
    "    # logPr = np.zeros(X.shape[0])\n",
    "    # for n in range(X.shape[0]):\n",
    "    #     for d in range(X.shape[1]):\n",
    "    #         logPr[n] = np.sum(np.sum(X[n,d]*np.log(mu) + 1-X[n,d]*np.log(1-mu)))    \n",
    "    return logPr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ml_nb_test(m, pc, X_test):\n",
    "    ##########################################################\n",
    "    ######################Your code Here######################\n",
    "    ##########################################################\n",
    "    return ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m, pc = ml_nb_train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ttest= ml_nb_test(m, pc, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print np.sum(T_true!=ttest)/X_test.shape[0]\n",
    "print np.count_nonzero(T_true!=ttest)/X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,10)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "for k in range(10):\n",
    "    im = Image.fromarray((m[k]).reshape(28,28)*255)\n",
    "    ax[k].axis('off')\n",
    "    ax[k].imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(10,10)\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "for k in range(10):\n",
    "    for i in range(10):\n",
    "        im = Image.fromarray((m[k]*np.random.rand(784)).reshape(28,28)*255)\n",
    "        ax[k, i].axis('off')\n",
    "        ax[k, i].imshow(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
