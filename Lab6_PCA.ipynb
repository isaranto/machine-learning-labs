{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab5 - Machine Learning\n",
    "# Principal Component Analysis - PCA\n",
    "## 1. Standard PCA - Analysis of Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in the dimensionality reduction lecture, PCA occurs as follows:\n",
    "* Calculate the mean vector $\\mu$ of the data\n",
    "* Normalize the data to have a zero mean:\n",
    "\n",
    "$$\\mathbf{X}_n = \\mathbf{X}_n - \\mu, \\quad n = 1,\\ldots,N$$\n",
    "* Construct the $d \\times d$ covariance matrix:\n",
    "$$\\mathbf{S} = \\frac{1}{N}\\sum_{n=1}^{N} x_n x^T$$\n",
    "where each $x_n$ is a column vector\n",
    "    * $\\mathbf{S}_{ii}$ (diagonal) is the variance of variable $i$\n",
    "    * $\\mathbf{S}_{ij}$ (off diagonal) is the covariance between variables $i$ and $j$\n",
    "    \n",
    "* Compute the eigenvalues and eigenvectors of the covariance matrix $\\mathbf{S}$\n",
    "* Keep the $k$ eigenvectors $\\mathbf{U}_{:k}$ corresponding to the $k$ largest eigenvalues (principal components)\n",
    "* Find the respresentation of the data in the reduced dimension \n",
    "$$ z_n = \\mathbf{U}^T x_n\\quad n = 1,\\ldots,N$$\n",
    "* Project the inputs into the space spanned by the principal components: $\\mathbf{X}_{reduced} = \\mathbf{X}_{c} \\mathbf{U}_{:k}$\n",
    "* Reconstruct the initial data $$ \\tilde{x_n}=\\mathbf{U}z_n +\\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    Loads the MNIST dataset.\n",
    "    \"\"\"\n",
    "    train_files = ['data/mnist/train%d.txt' % (i,) for i in range(10)]\n",
    "    tmp = []\n",
    "    for i in train_files:\n",
    "        with open(i, 'r') as fp:\n",
    "            tmp += fp.readlines()\n",
    "    X = np.array([[j for j in i.split(\" \")] for i in tmp], dtype='int')    \n",
    "    return X\n",
    "\n",
    "def plot_mnist(X, ind =[]):\n",
    "    if not len(ind):\n",
    "        ind = np.random.permutation(X.shape[0])\n",
    "    f = plt.figure()\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(15)\n",
    "    for i in range(100):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X[ind[i]].reshape(28,28).real, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_classic(X, M):\n",
    "    #################################################### \n",
    "    #################################################### \n",
    "    ###################Your code here################### \n",
    "    #################################################### \n",
    "    #################################################### \n",
    "    return Z, U, Lambdas, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigsort(A):\n",
    "    eigvals, U = np.linalg.eig(A)\n",
    "    # sort eigenvalues in descending order (we multiply the matrix with -1)\n",
    "    order = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[order]\n",
    "    #re-arrange the eigenvectors\n",
    "    U = U[:,order]\n",
    "    return U, eigvals    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = load_mnist()\n",
    "# Low dimensionality (how many eigenvectors) \n",
    "M = 50\n",
    "Z, eigvecs, eigvals, mu = pca_classic(X, M) \n",
    "X_rec = Z.dot(eigvecs.T) + mu\n",
    "ind = np.random.permutation(X.shape[0])\n",
    "plot_mnist(X, ind)\n",
    "plot_mnist(X_rec, ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "if M<=100:\n",
    "    for i in range(M):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(eigvecs[:,i].reshape(28,28).real, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Using Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_faces():\n",
    "    \"\"\"\n",
    "    Load eigenfaces\"\"\"\n",
    "    data = scipy.io.loadmat('data/faces.mat')\n",
    "    X =  data['D'].T    \n",
    "    return X\n",
    "\n",
    "def plot_faces(X, ind = []):\n",
    "    if not len(ind):\n",
    "        ind = np.random.permutation(X.shape[0])\n",
    "    f = plt.figure()\n",
    "    f.set_figheight(15)\n",
    "    f.set_figwidth(15)\n",
    "    for i in range(100):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X[ind[i]].reshape(92,112).T.real, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In a simillar manner we can use Singular Value Decomposition (SVD) to perform PCA. We decompose X\n",
    "using SVD:\n",
    "$$ X= \\mathbf{U}\\mathbf{S}\\mathbf{V}^T$$\n",
    " where\n",
    "<ul>\n",
    "  <li> $\\mathbf{U}: n \\times n$  matrix has as columns the eigenvectors of $\\mathbf{X} \\mathbf{X^T}$ </li>\n",
    "  <li> $\\mathbf{\\Sigma}: n \\times d$ is a diagonal matrix with the singular values of $\\mathbf{X}$ in the diagonal (= square roots of $\\mathbf{X} \\mathbf{X^T}$ eigenvalues) </li>\n",
    "  <li> $\\mathbf{V}^T: d \\times d$  matrix has as columns the eigenvectors of $\\mathbf{X^T} \\mathbf{X}$ </li>\n",
    "</ul>\n",
    "By using the function np.linalg.svd we calculate the decomposition of X. If the parameter \"full_matrices\" is set to True (default), U and V have the shapes ($N \\times N$) and ($D \\times D$), respectively. Otherwise, the shapes are ($N \\times K$) and ($K \\times D$), respectively, where K = min(N, D)\n",
    "So in our example we have: $\\mathbf{V}^T: d \\times n$\n",
    "\n",
    "The eigenvalues of the covariance matrix are given by $$ \\lambda_{i} = \\frac{S_{i}^2}{N} $$\n",
    "\n",
    "The upside of SVD is that we calculate the eigenvalues of the covariance matrix without calculating the matrix itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_svd(X, M):\n",
    "    N, D = X.shape    \n",
    "    #################################################### \n",
    "    #################################################### \n",
    "    ###################Your code here################### \n",
    "    #################################################### \n",
    "    ####################################################     \n",
    "    U, S, V = np.linalg.svd(X, full_matrices=False)\n",
    "    print \"U\", U.shape\n",
    "    print \"S\", S.shape\n",
    "    print \"V\", V.shape\n",
    "    #################################################### \n",
    "    #################################################### \n",
    "    ###################Your code here################### \n",
    "    #################################################### \n",
    "    ####################################################    \n",
    "    return Z, V, eigvals, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = load_faces()\n",
    "# Low dimensionality (how many eigenvectors) \n",
    "M = 100\n",
    "# X_rec, lamdas = pca_classic(X, M)\n",
    "Z, eigvecs, eigvals, mu = pca_svd(X, M)\n",
    "X_rec = Z.dot(eigvecs.T) + mu\n",
    "ind = np.random.permutation(X.shape[0])\n",
    "plot_faces(X, ind)\n",
    "plot_faces(X_rec, ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets plot the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "f.set_figheight(15)\n",
    "f.set_figwidth(15)\n",
    "if M<=100:\n",
    "    for i in range(M):\n",
    "        plt.subplot(10, 10, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(eigvecs[:,i].reshape(92,112).T.real, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How many Principal components?\n",
    "\n",
    "A simple way to select the number of the components in which we will represent our data is to set a threshold for  proportion of the variance that explains our reconstructed data will hold.\n",
    "In typical scenarios the chosen components should explain at least 85% of the variance (\n",
    " \n",
    " $$ \\frac{\\sum_{i=1}^M \\lambda_{i}} {\\sum_{i=1}^N \\lambda_{i}} \\geq 0.85$$\n",
    "85% of the variance is retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_mnist()\n",
    "# Low dimensionality (how many eigenvectors) \n",
    "M = 50\n",
    "Z, eigvecs, eigvals, mu = pca_classic(X, M) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(eigvals[:M])/np.sum(eigvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the proportion of explained variance for components\n",
    "x = range(X.shape[1])\n",
    "y = [np.sum(eigvals[:m])/np.sum(eigvals)*100 for m in x]\n",
    "plt.plot(x, y)\n",
    "plt.plot(x,[85]*len(x), 'r--')\n",
    "plt.ylabel(\"Proportion of Variance (%)\")\n",
    "plt.xlabel(\"# of components\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
